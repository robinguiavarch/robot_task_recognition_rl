# ü§ñ Multi-Agent Reinforcement Learning: Symbolic Reasoning Meets Robotic Execution

## üß† Project Description

This project showcases a **multi-agent reinforcement learning pipeline** where two agents work together to complete a robotic manipulation task.

### Agent 1: Pattern-to-Instruction Reasoning

**Agent 1** is a decision-making agent operating on **symbolic event streams** generated by the custom `OpenTheChests` Gym environment. Its objective is to interpret event sequences and output a 3-bit binary instruction indicating which chest (0, 1, or 2) should be manipulated.

To progressively enhance its ability to process sequential information, we developed and evaluated **three distinct approaches**:

1. **Simple Mapping** ‚Äì One event ‚Üí One action (baseline)
2. **Temporal Window** ‚Äì Finite-length event window ‚Üí Action
3. **Sequence Modeling** ‚Äì LSTM / Transformer-based architecture for long-term pattern recognition

Agent 1 is trained using **Q-learning** with deep neural network approximators (e.g., Double DQN), taking as input flattened sliding windows of encoded events.

### Agent 2: Robotic Execution

**Agent 2** is a robot control agent operating in a **physics-based environment** powered by Gymnasium and simulating a **Franka Emika Panda arm**. It receives instructions from Agent 1 and learns to:

- Move toward a specified box (red, blue, or green)
- Grasp the box with the gripper
- Lift it above a 12 cm threshold
- Release it in the target area

The agent is trained using state-of-the-art continuous control algorithms from **Stable-Baselines3** (e.g., PPO, SAC), with a carefully designed **reward shaping** strategy spanning multiple interaction phases.

### Joint Execution: Brain and Body

The final system connects the two agents:

- **Agent 1 acts as the brain**, reading symbolic sequences and deciding which box to target.
- **Agent 2 acts as the body**, executing the physical motion required to complete the task.

This modular architecture allows symbolic abstraction and low-level control to coexist and cooperate, simulating a real-world intelligent robotic system.

---

## üìÅ Project Structure

The repository is organized into **three directories**, each corresponding to a component of the project:

- `agent1_patterns_chests_to_reach`: Event-to-action reasoning (Agent 1)
- `agent2_physically_reach_chest`: Robotic manipulation (Agent 2)
- `agent1_and_agent2`: Integration and full pipeline execution

Each directory includes a dedicated Jupyter notebook demonstrating training and evaluation of the respective agent.

---

## üì¶ Agent 1 Directory Structure

The `agent1_patterns_chests_to_reach` folder is divided into **three subdirectories**, each representing a modeling approach:

- `approach1_simple_mapping`
- `approach2_temporal_window`
- `approach3_sequence_modeling`

Inside each approach folder, the following components are structured for modularity:

- `encoder/` ‚Äì Functions to encode environment observations into numerical vectors
- `agents/` ‚Äì Reinforcement learning models implemented as PyTorch classes
- `training/` ‚Äì Training scripts for each model
- `evaluation/` ‚Äì Scripts to test and benchmark trained agents
- `plot_results/` ‚Äì Visualization scripts for losses, success rates, and reward curves

This layout ensures a clean and scalable design for experimenting with different input representations and model architectures.

---

## üìì Agent 2 and Agent 1+2 Execution

The folders `agent2_physically_reach_chest` and `agent1_and_agent2` each contain **a single Jupyter notebook** covering the full experimental setup for that part:

- Environment setup
- Training loop
- Evaluation and visualization

To run these experiments, simply open and execute the notebook step by step.

---

## üë• Authors

This project was developed by:

- **Mathieu Delarue**
- **Fran√ßois Xavier Morel**
- **Laury Magne**
- **Robin Guiavarch**
